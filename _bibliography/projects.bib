---
---

@research{hsu2023graphPROTACs,
      title={GraphPROTACs: Out Of Distribution DC50 Prediction}, 
      research_location={Celeris Therapeutics},
      year={3.2023 - 1.2024},
      preview={protac.png},
      paper={https://drive.google.com/file/d/1Qd66_jYEx6rRiMa96QrRzNRNy2e5RdWd/view?usp=sharing},
      description={Curated a PROTAC dataset and proposed GraphPROTACs, achieving state-of-the-art generalizability (AUROC increased from 55% to 75%) and enhanced model interpretability, with further improvements from training on synthetic data.},
      selected={true}
}

@research{hsu2021adversarial,
      title={Adversarial Attacks on Reinforcement Learning Agents Trained with Self-Play}, 
      research_location={Technical University of Munich},
      year={11.2020 - 3.2021},
      preview={pong.gif},
      paper={https://drive.google.com/file/d/1sVzKuJNBUo_a_Cl65FjUIhT-aBREi54x/view?usp=sharing},
      code={https://github.com/PavelCz/rl-adversarial-attack},
      description={Researched training RL agents in multi-agent environments and examined their robustness through observation-based and adversarial policy attacks.},
      selected={true}
}


@research{nn,
      title={Neural Network Interpretability}, 
      research_location={Technical University of Munich},
      year={4.2020 - 8.2020},
      preview={IMG_6989.png},
      paper={https://drive.google.com/file/d/13KlC5WVTZyyZ82514RnuNOq7ifN63Sc1/view?usp=sharing},
      code={https://github.com/hans66hsu/nn_interpretability},
      description={Researched deep learning interpretability methods and explored their ability to interpret neural network prediction uncertainty.},
      github_star={https://img.shields.io/github/stars/hans66hsu/nn_interpretability},
      selected={true}
}




